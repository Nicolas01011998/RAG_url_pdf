# RAG Web+PDF QA

A Question Answering (QA) system in Italian based on Retrieval-Augmented Generation (RAG). It can answer user queries by leveraging content from **PDF documents** and/or **web URLs**.

## Features

- üìÑ Advanced PDF extraction (uses 3 loaders: `PyPDFium2`, `PDFMiner`, `PyPDF`)
- üåê Content loading from URLs (`WebBaseLoader`)
- ‚úÇÔ∏è Automatic text splitting using `RecursiveCharacterTextSplitter`
- üß† Semantic embeddings via `OllamaEmbeddings` (`nomic-embed-text` model)
- üõ¢Ô∏è Indexing and retrieval with `Chroma` vector store
- üí¨ Answer generation with a local LLM (`gemma3`)
- üóÇÔ∏è Metadata enhancement by extracting section titles from documents
- üåê User interface built with [Gradio](https://gradio.app/)

## How It Works

- **Input**: The user provides a URL and/or a PDF file  
- **Extraction**: The content is cleaned, split, and converted into embeddings  
- **Indexing**: The text chunks are stored in a Chroma vectorstore  
- **Retrieval**: The system selects the most relevant documents for the question  
- **LLM**: A contextual answer is generated using the `gemma3` model

## Notes

- Only supports `.pdf` files smaller than 50MB  
- Answers are based solely on the provided content (Web + PDF)  
- Questions must be asked in **Italian**  
- The system clearly states when no relevant information is found in the context

## Project 
The project has the following structure:
```plaintext
.
‚îú‚îÄ‚îÄ rag_llm.py                # Code
‚îú‚îÄ‚îÄ requirements.txt          # Requirements 
‚îî‚îÄ‚îÄ README.md             

```

## Environment
### 1. Conda environment
To run the script, it is first necessary to create a Conda environment with Python 3.12 and a virtual environment to install the required libraries.
The Conda environment is used to set up the Python version.
To create a Conda environment, run the following command:
```
sudo apt apt-get install tesseract-ocr tesseract-ocr-ita libmagic-dev poppler-utils
conda create --name conda_python_3_12 python=3.12
```
To activate the Conda environment:
```
conda activate conda_python_3_12
```

### 2. Virtual environment
After activating the Conda environment, it is necessary to create a virtual environment to install the required libraries.
```
python3 -m venv venv
```
Now, activate the virtual environment:
```
source venv/bin/activate
```
Finally, install the necessary libraries using pip.
```
pip install -r requirements.txt
```

### 3. Ollama 
Download and install Ollama on Linux
```
curl -fsSL https://ollama.com/install.sh | sh
```

Create a systemd service to start Ollama automatically on boot. Create the service file:
```
sudo nano /etc/systemd/system/ollama.service
```

And paste the following:
```
[Unit]
Description=Ollama Service
After=network.target

[Service]
ExecStart=/usr/local/bin/ollama serve
Restart=always
User=YOUR_USERNAME
WorkingDirectory=/home/YOUR_USERNAME
Environment=PATH=/usr/local/bin:/usr/bin:/bin

[Install]
WantedBy=multi-user.target
```
+ Replace YOUR_USERNAME with your Linux username (whoami)
+ Make sure /usr/local/bin/ollama is the correct path (which ollama to verify)

Enable and start the service:
```
sudo systemctl daemon-reexec
sudo systemctl daemon-reload
sudo systemctl enable ollama
sudo systemctl start ollama
```

Check the service status:
```
systemctl status ollama
```

Pull models:
```
ollama pull nomic-embed-text
ollama pull gemma3:1b
```

## Run script
```
python rag_llm.py
```

## Example of Visualization 

This figure shows an example of the second pipeline is when object detection was performed only for the classes: book, potted plant, and vase.

<img src="Example.png" >
